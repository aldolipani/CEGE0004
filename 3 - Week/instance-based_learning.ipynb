{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Instance Based Learning\n",
    "\n",
    "In this notebook you will learn how to implement the k-Nearest Neighbors (kNN) algorithm in Python and learn how to\n",
    "use kNN and SVN algorithms in scikit-learn.\n",
    "\n",
    "## The Dataset\n",
    "\n",
    "In this notebook you will be working with the [Wine dataset](https://archive.ics.uci.edu/ml/datasets/Wine).\n",
    "This dataset is the result of a chemical analysis of wines\n",
    "grown in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents\n",
    "found in each of the three types of wines.\n",
    "\n",
    "This is a good dataset for testing classifiers. In the following table we find some properties of this dataset.\n",
    "\n",
    "|Property|Value|\n",
    "|--|--|\n",
    "|Classes|3|\n",
    "|Samples per class|~59|\n",
    "|Samples total|178|\n",
    "|Dimensionality|13|\n",
    "|Features|positive, natural and real|\n",
    "\n",
    "Each example contains a class identifier and 13 attributes representing the outcome of the analysis performed on the\n",
    "wine samples.\n",
    "\n",
    "We will download this dataset directly from the UCI repository using Pandas. Note that this dataset does not have a\n",
    "header which means that we need to provide the column names manually. This header comes from reading the content of this\n",
    "[file](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.names)\n",
    "(you can open this file with a text editor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data'\n",
    "\n",
    "names = ['class', # label\n",
    "         'alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total phenols', 'flavanoids',\n",
    "         'non-flavanoid phenols', 'proanthocyanins', 'color_intensity', 'hue', 'protein_content', 'proline']\n",
    "\n",
    "df = pd.read_csv(dataset_url, names=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's first have a look at the content of this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's also have a look at some statistics of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocessing the Dataset\n",
    "\n",
    "This dataset will require some preprocessing. First, since we would like to test how any of the learning algorithms\n",
    "used later perform on unseen data. We split the dataset into a training set and a test set.\n",
    "\n",
    "To do this we first randomize the data. We do this in order to make sure to select an unbiased set of examples\n",
    "for the test set. Notice that in this case the training is sorted based on the target `class`, which makes the\n",
    "initial randomization necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now separate the target `class` from the rest of the attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# this sets the numpy to print numbers with float precision (this setting affects only the prints not the actual values)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "ys, xs = np.split(df.values, [1], axis=1)\n",
    "ys = ys.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "And select 80% of the dataset for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_train = len(xs) * 80 // 100\n",
    "xs_train, xs_test = np.split(xs, [n_train], axis=0)\n",
    "ys_train, ys_test = np.split(ys, [n_train], axis=0)\n",
    "\n",
    "print('training set shape:\\t', xs_train.shape)\n",
    "print('test set shape:\\t\\t', xs_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, we note that attribute values span across various ranges: Some attributes have a much wider range than others.\n",
    "Since the learning algorithms that we will be using later are based on some form of distance function,\n",
    "this variance in the ranges of the attributes may bias the learning algorithm towards examples that look closer in\n",
    "dimensions with a wider range because those attributes will dominate the distance functions.\n",
    "In order to mitigate this bias we perform a normalization across the features. In this case we will perform a\n",
    "standardization (aka Z-score):\n",
    "\n",
    "\\begin{equation}\n",
    "z = \\frac{x-\\mu}{\\sigma}\n",
    "\\end{equation}\n",
    "\n",
    "Any other normalization that achieves a similar result is also fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mu = np.mean(xs_train, axis=0)\n",
    "sigma = np.std(xs_train, axis=0)\n",
    "\n",
    "xs_train = (xs_train - mu)/sigma\n",
    "xs_test = (xs_test - mu)/sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that the normalization should be computed on the training set and not on the original dataset. This in order to\n",
    "better simulate unseen data. The normalization, together with any preprocessing step that involves statistics over\n",
    "the dataset, should also be considered as belonging to the hyper-parameters of the learning algorithm.\n",
    "\n",
    "After having performed the normalization, if we now compute the mean of the preprocessed training set, we should see\n",
    "that now this mean vector contains only zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(xs_train, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If we have properly sampled the dataset, we should get a mean vector for the test set that contains close to zero\n",
    "values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(xs_test, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The Nearest Neighbor Algorithm\n",
    "\n",
    "We will now implement the Nearest Neighbor (NN) algorithm in Python from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NN:\n",
    "\n",
    "    def __init__(self, distance):\n",
    "        self.training_examples = []\n",
    "        self.distance = distance\n",
    "\n",
    "    def add_example(self, x, y):\n",
    "        \"\"\"\n",
    "        Add one example to the list of training examples.\n",
    "        :param x: The vector of feature values\n",
    "        :param y: The label associated to this example\n",
    "        \"\"\"\n",
    "        self.training_examples.append((x, y))\n",
    "\n",
    "    def add_examples(self, xs, ys):\n",
    "        \"\"\"\n",
    "        Add a list of examples to the list of training examples.\n",
    "        :param xs: A list of vectors of fature values\n",
    "        :param ys: A list of labels associated to the examples\n",
    "        \"\"\"\n",
    "        for x, y in zip(xs, ys):\n",
    "            self.add_example(x, y)\n",
    "\n",
    "    def closest_training_example(self, x_q):\n",
    "        y_closest = None\n",
    "        x_closest = None\n",
    "        min_score = float('inf')\n",
    "        # find closest example\n",
    "        for x, y in self.training_examples:\n",
    "            score = self.distance(x_q, x)\n",
    "            if score < min_score:\n",
    "                min_score = score\n",
    "                x_closest = x\n",
    "                y_closest = y\n",
    "\n",
    "        return x_closest, y_closest\n",
    "\n",
    "    def classify(self, xq):\n",
    "        _, y_hat = self.closest_training_example(xq)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In order to instantiate this classifier we need to define a distance function. Since we are dealing with continuous\n",
    "features we will define the euclidean distance. You are invited to develop and test another distance measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def euclidean_distance(x_1, x_2):\n",
    "    res = 0\n",
    "    for a_1, a_2 in zip(x_1, x_2):\n",
    "        res += (a_1 - a_2) ** 2\n",
    "    res **= 0.5\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The euclidean distance of the points (0, 0) and (1, 1) should be $\\sqrt{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "euclidean_distance([0, 0], [1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now instantiate the NN classifier and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nn_clf = NN(euclidean_distance)\n",
    "\n",
    "nn_clf.add_examples(xs_train, ys_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To evaluate how this classifier performs on the test set we will measure its accuracy. Note that evaluating the\n",
    "accuracy on the training set is pointless because this will always be 1 by definition. We will now do also this only for\n",
    "instructive purposes.\n",
    "\n",
    "We now define the accuracy measure. Remember that the accuracy is equal to the proportion of examples that the\n",
    "classifier predicted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(ys, ys_hat):\n",
    "    res = 0\n",
    "    for y, y_hat in zip(ys, ys_hat):\n",
    "        if y == y_hat:\n",
    "            res += 1\n",
    "    res /= len(ys)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now test the classifier on both training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ys_train_pred = []\n",
    "for x in xs_train:\n",
    "    y_hat = nn_clf.classify(x)\n",
    "    ys_train_pred.append(y_hat)\n",
    "\n",
    "ys_test_pred = []\n",
    "for x in xs_test:\n",
    "    y_hat = nn_clf.classify(x)\n",
    "    ys_test_pred.append(y_hat)\n",
    "\n",
    "print('Train accuracy of NN', accuracy(ys_train, ys_train_pred))\n",
    "print('Test accuracy of NN', accuracy(ys_test, ys_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's now compare the test result of this classifier to a random classifier. Is this NN classifier any better?\n",
    "Remember that a random classifier would correctly predict the class one third of the times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ys_test_pred_random = np.random.randint(1, 4, len(ys_test))\n",
    "print('Test accuracy of a random classifier', accuracy(ys_test, ys_test_pred_random))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's have a look at the result of a random classifier by repeating this experiment many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "accuracies = []\n",
    "for _ in range(10000):\n",
    "    ys_test_pred_random = np.random.randint(1, 4, len(ys_test))\n",
    "    accuracies.append(accuracy(ys_test, ys_test_pred_random))\n",
    "\n",
    "plt.hist(accuracies)\n",
    "plt.show()\n",
    "\n",
    "print('Expected accuracy of a random classifier', np.mean(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Does this accuracy make sense? Is this accuracy similar to the one you had predicted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# The k-Nearest Neighbor Algorithm\n",
    "\n",
    "We will now extend the NN algorithm to develop the k-Nearest Neighbor (kNN) algorithm.\n",
    "By extending the NN algorithm we avoid repeating the training code. This is in fact the same.\n",
    "In the classification method, in order to avoid the sorting of\n",
    "all the examples after having measured their score, we will make use of priority queues, which allow us to keep track\n",
    "only of the first $k$-nearest examples, making the code more efficient. You are free to implement the version where\n",
    "first all the examples are scored, then sorted and selected. These two versions, if correctly implemented,\n",
    "should produce to the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from statistics import mode\n",
    "from heapq import heappush, heappushpop\n",
    "\n",
    "class KNN(NN):\n",
    "\n",
    "    def __init__(self, distance):\n",
    "        super().__init__(distance)\n",
    "\n",
    "    def closest_training_examples(self, x_q, k=1):\n",
    "        k_nearest = []\n",
    "\n",
    "        # initialize an heap with k elements\n",
    "        for x, y  in self.training_examples[:k]:\n",
    "            score = self.distance(x_q, x)\n",
    "            heappush(k_nearest, (-score, (x, y)))\n",
    "\n",
    "        # find the k-nearest example\n",
    "        for x, y in self.training_examples[k:]:\n",
    "            score = self.distance(x_q, x)\n",
    "            heappushpop(k_nearest, (-score, (x, y)))\n",
    "\n",
    "        # we no longer need to keep the score\n",
    "        res = [(x, y) for _, (x, y) in k_nearest]\n",
    "        return res\n",
    "\n",
    "    def classify(self, x_q, k = 1):\n",
    "        # find the k closest\n",
    "        k_nearest_xs, k_nearest_ys = zip(*self.closest_training_examples(x_q, k))\n",
    "        # return the mode\n",
    "        return mode(k_nearest_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now train and test this algorithm in the same way we did for the NN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "knn_clf = KNN(euclidean_distance)\n",
    "\n",
    "knn_clf.add_examples(xs_train, ys_train)\n",
    "\n",
    "ys_train_pred = []\n",
    "for x in xs_train:\n",
    "    y_hat = knn_clf.classify(x)\n",
    "    ys_train_pred.append(y_hat)\n",
    "\n",
    "ys_test_pred = []\n",
    "for x in xs_test:\n",
    "    y_hat = knn_clf.classify(x)\n",
    "    ys_test_pred.append(y_hat)\n",
    "\n",
    "print('Train accuracy of kNN', accuracy(ys_train, ys_train_pred))\n",
    "print('Test accuracy of kNN', accuracy(ys_test, ys_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Nothing has changed with respect to the NN algorithm because we have implicitly used $k=1$. Let's now try $k=5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ys_train_pred = []\n",
    "for x in xs_train:\n",
    "    y_hat = knn_clf.classify(x, 5)\n",
    "    ys_train_pred.append(y_hat)\n",
    "\n",
    "ys_test_pred = []\n",
    "for x in xs_test:\n",
    "    y_hat = knn_clf.classify(x, 5)\n",
    "    ys_test_pred.append(y_hat)\n",
    "\n",
    "\n",
    "print('Train accuracy of kNN', accuracy(ys_train, ys_train_pred))\n",
    "print('Test accuracy of kNN', accuracy(ys_test, ys_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It seems that considering more points did not help. Note that this time the training accuracy has changed. Can you\n",
    "explain why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## kNN in Scikit-Learn\n",
    "\n",
    "We will now learn how to use the kNN implementation of scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5, metric='euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "knn_clf.fit(xs_train, ys_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now evaluate the result of this classifier. Here, we should not see any\n",
    "difference with respect to the results obtained above with our implementation of the kNN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "ys_test_pred = knn_clf.predict(xs_test)\n",
    "\n",
    "print('Test accuracy of kNN', accuracy_score(ys_test, ys_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's now try the cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "knn_clf = KNeighborsClassifier(n_neighbors=1, metric='cosine')\n",
    "\n",
    "knn_clf.fit(xs_train, ys_train)\n",
    "\n",
    "ys_test_pred = knn_clf.predict(xs_test)\n",
    "\n",
    "print('Test accuracy of kNN', accuracy_score(ys_test, ys_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Try other $n$ values. Can you find a better one? The danger of doing this hyper-parameter exploration using the test set is\n",
    "that we may overfit these hyper-parameters on the test set.\n",
    "To avoid this, it is better to find the best hyper-parameter values via a validation strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To find these hyper-parameter values we can exploit the grid search of scikit-learn. This will perform a k-fold\n",
    "cross-validation on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [{\n",
    "    'weights': [\"uniform\", \"distance\"],\n",
    "    'n_neighbors': range(1, 11),\n",
    "    'metric':['euclidean', 'manhattan', 'cosine']}]\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn_clf, param_grid, cv=5, verbose=2)\n",
    "grid_search.fit(xs_train, ys_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can now see what are the best hyper-parameter values found by the cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's now try this hyper-parameters on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "knn_clf = KNeighborsClassifier(metric='cosine', n_neighbors=4, weights='distance')\n",
    "\n",
    "knn_clf.fit(xs_train, ys_train)\n",
    "\n",
    "ys_train_pred = knn_clf.predict(xs_train)\n",
    "ys_test_pred = knn_clf.predict(xs_test)\n",
    "\n",
    "print('Train accuracy of kNN', accuracy(ys_train, ys_train_pred))\n",
    "print('Test accuracy of kNN', accuracy(ys_test, ys_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The accuracy measured on the test set is now a better estimate of the accuracy we would expect on unseen examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "Here I will introduce you how to use the Support Vector Machine (SVM) implementation of scikit-learn.\n",
    "\n",
    "Note how we are setting the $C$ hyper-parameter of SVM. $C$ controls the trade-off between having a small and strict\n",
    "margin and a wider and loose margin. Following we will set $C$ to infinity which makes the margin infinitely strict.\n",
    "This means that based on the dataset, the fitting of the SVM may fail if the training algorithm fails to separate all\n",
    "the training examples perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf = SVC(kernel=\"linear\", C=float(\"inf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now train this classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "svm_clf.fit(xs_train, ys_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The training went well, which means that the SVM training algorithm managed to perfectly fit the training examples.\n",
    "We can now verify this on the training set. We will also test the performance of this classifier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ys_train_pred = svm_clf.predict(xs_train)\n",
    "ys_test_pred = svm_clf.predict(xs_test)\n",
    "\n",
    "print('Train accuracy of SVM', accuracy(ys_train, ys_train_pred))\n",
    "print('Test accuracy of SVM', accuracy(ys_test, ys_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "How would you find the best hyper-parameter C value? Try re-implement the code used to validate the kNN\n",
    "classifier above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
