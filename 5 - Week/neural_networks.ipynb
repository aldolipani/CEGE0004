{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Networks\n",
    "\n",
    "In this notebook you will learn how to implement the perceptron and a sigmoid unit in Python and how to use PyTorch to\n",
    "implement the Multilayer perceptron (MLP).\n",
    "\n",
    "## The Perceptron and Sigmoid Unit\n",
    "\n",
    "In this section we will be working with the perceptron. This is not commonly used in\n",
    "practice but understanding its inner working will help you understand more complicated neural network architectures.\n",
    "\n",
    "To test this algorithm we will use a simulated dataset.\n",
    "\n",
    "## The Dataset\n",
    "\n",
    "To generate the dataset we will start from a bi-variate normal distribution with mean $(0, 0)$ and\n",
    "variance $I_2$. We will now sample 1000 examples. Note that before doing that we create a numpy\n",
    "array of size (n, 3) filled with 1s. This is done in order to avoid the bias term in the perceptron later and\n",
    "make the algorithm simpler."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# number of examples to generate\n",
    "n = 1000\n",
    "# create an array of ones of size n x 3\n",
    "xs = np.ones((n, 3))\n",
    "# use a bi-variate normal distribution to generate the examples\n",
    "xs[:, 1:] = np.random.multivariate_normal(mean = [0, 0], cov=np.eye(2), size = n)\n",
    "\n",
    "xs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now plot this bi-dimensional dataset to see how it looks like."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(xs[:,1], xs[:,2])\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To generate a binary target function we will first generate a random hyperplane and use that with the previously\n",
    "generated data to separate it into two classes.\n",
    "\n",
    "\\begin{equation}\n",
    "w_0 + w_1 x_1 + w_2 x_2 > 0,\\quad w_0 + w_1 x_1 + w_2 x_2 \\leq 0\n",
    "\\end{equation}\n",
    "\n",
    "When this returns a positive value we will assign 1, when this returns a negative value with zero included we will\n",
    "assign -1."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ws = np.random.random(3)\n",
    "\n",
    "ws"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To generate the target value we perform the following transformation:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ys = 2 * (ws @ xs.transpose() > 0) - 1\n",
    "\n",
    "# we print the first 10 results\n",
    "ys[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now inspect this target function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.scatter(xs[:,1], xs[:,2], c=ys)\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have now generated a linearly separable dataset.\n",
    "\n",
    "## The Perceptron\n",
    "\n",
    "Now we implement in Python the perceptron."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "\n",
    "    def __init__(self, input_size=3):\n",
    "        # initialize weights to a small random value\n",
    "        self.ws = np.random.random(input_size)/10\n",
    "\n",
    "    def classify(self, x):\n",
    "        return 2 * (self.ws @ x > 0) - 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now instantiate a perceptron with an input size of 3 and test it using the first example of the previously generated\n",
    "dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pr_clf = Perceptron(3)\n",
    "\n",
    "pr_clf.classify(xs[0, :])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To train this we need to implement the update rule for the Perceptron.\n",
    "\n",
    "But first let's test this randomly initialized perceptron by evaluating its accuracy."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def accuracy(ys, hat_ys):\n",
    "    return np.mean(ys == hat_ys)\n",
    "\n",
    "hat_ys = pr_clf.classify(xs.transpose())\n",
    "\n",
    "plt.scatter(xs[:, 1], xs[:, 2], c=hat_ys)\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.show()\n",
    "\n",
    "print('Accuracy of a randomly initialized Perceptron', accuracy(ys, hat_ys))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will now first develop the Mean Squared Error (MSE) function, which will be used as a loss function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def mean_squared_error(ys, hat_ys):\n",
    "    return np.mean((ys - hat_ys)**2)\n",
    "\n",
    "print('MSE of a randomly initialized Perceptron', mean_squared_error(ys, hat_ys))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we develop the gradient descent algorithm for the Perceptron."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gradient_descent_for_perceptron(model, xs, ys, eta=0.00005, epochs=20):\n",
    "    # this part is used to log the training error\n",
    "    hat_ys = model.classify(xs.transpose()) # compute prediction\n",
    "    error = mean_squared_error(ys, hat_ys)\n",
    "    train_error = [error]\n",
    "\n",
    "    # this is the training loop\n",
    "    for _ in range(epochs): # termination condition\n",
    "        delta_ws = np.zeros(xs.shape[1]) # initialize delta weights to zero\n",
    "        # for example we compute accumulate the gradient\n",
    "        for x, y in zip(xs, ys):\n",
    "            hat_y = model.classify(x)\n",
    "            delta_ws += eta * (y - hat_y) * x\n",
    "        # we update the weights of the perceptron\n",
    "        model.ws = model.ws + delta_ws\n",
    "\n",
    "        # compute its error for logging purposes\n",
    "        hat_ys = model.classify(xs.transpose()) # compute model\n",
    "        error = mean_squared_error(ys, hat_ys)\n",
    "        train_error.append(error)\n",
    "\n",
    "    return train_error\n",
    "\n",
    "train_error = gradient_descent_for_perceptron(pr_clf, xs, ys, eta=0.00005, epochs=20)\n",
    "\n",
    "plt.plot(train_error)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the figure above we see the training curve of the perceptron. We can see that at each epoch the\n",
    "perceptron makes fewer mistakes, until it converged to 0. Try to play with the learning rate ($\\eta$)\n",
    "and the number of epochs to see how this curve changes. For example test what happens when the learning rate\n",
    "is much larger.\n",
    "\n",
    "If we now evaluate the trained Perceptron we should obtain a perfect classification (accuracy equal to 1)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hat_ys = pr_clf.classify(xs.transpose())\n",
    "\n",
    "plt.scatter(xs[:, 1], xs[:, 2], c=hat_ys)\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.show()\n",
    "\n",
    "print('Accuracy of the trained Perceptron', accuracy(ys, hat_ys))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training a Sigmoid Unit\n",
    "\n",
    "In order to implement the Sigmoid unit we first need to define the sigmoid function.\n",
    "This a special version of the sigmoid function because it works with vectors."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "print('original vector\\t\\t\\t', xs[0, :])\n",
    "print('after applying the sigmoid\\t', sigmoid(xs[0, :]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SigmoidUnit:\n",
    "\n",
    "    def __init__(self, input_size=3):\n",
    "        # initialize weights to a small random value\n",
    "        self.ws = np.random.random(input_size)/10\n",
    "\n",
    "    def classify(self, x):\n",
    "        return  sigmoid(self.ws @ x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now instantiate the Sigmoid unit with an input size of 3 and test it using the first example of the previously generated\n",
    "dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "su_clf = SigmoidUnit(3)\n",
    "\n",
    "su_clf.classify(xs[0, :])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Please notice that the Sigmoid unit returns a value between 0 and 1. This has implications on the label that we need to\n",
    "provide when training and on its output that needs to be binarized by setting a thresholt."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hat_ys = su_clf.classify(xs.transpose())\n",
    "\n",
    "plt.scatter(xs[:, 1], xs[:, 2], c=hat_ys)\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now set the threshold to 0.5."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hat_ys = su_clf.classify(xs.transpose()) > 0.5\n",
    "\n",
    "plt.scatter(xs[:, 1], xs[:, 2], c=hat_ys)\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.show()\n",
    "\n",
    "print('Accuracy of a randomly initialized Perceptron', accuracy(ys, hat_ys))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To train this we need to implement the update rule for the Sigmoid unit."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gradient_descent_for_sigmoid_unit(model, xs, ys, eta=0.00005, epochs=20):\n",
    "    # this part is used to log the training error\n",
    "    hat_ys = model.classify(xs.transpose()) # compute prediction\n",
    "    error = mean_squared_error(ys, hat_ys)\n",
    "    train_error = [error]\n",
    "\n",
    "    # this is the training loop\n",
    "    for _ in range(epochs): # termination condition\n",
    "        delta_ws = np.zeros(xs.shape[1]) # initialize delta weights to zero\n",
    "        # for example we compute accumulate the gradient\n",
    "        for x, y in zip(xs, ys):\n",
    "            hat_y = model.classify(x)\n",
    "            delta_ws += eta * hat_y * (1 - hat_y) * (y - hat_y) * x\n",
    "        # we update the weights of the perceptron\n",
    "        model.ws = model.ws + delta_ws\n",
    "\n",
    "        # compute its error for logging purposes\n",
    "        hat_ys = model.classify(xs.transpose()) # compute model\n",
    "        error = mean_squared_error(ys, hat_ys)\n",
    "        train_error.append(error)\n",
    "\n",
    "    return train_error\n",
    "\n",
    "ys_for_su = ys > 0\n",
    "train_error = gradient_descent_for_sigmoid_unit(su_clf, xs, ys_for_su, eta=0.05, epochs=100)\n",
    "\n",
    "plt.plot(train_error)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hat_ys = su_clf.classify(xs.transpose())\n",
    "\n",
    "plt.scatter(xs[:, 1], xs[:, 2], c=hat_ys)\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hat_ys = su_clf.classify(xs.transpose()) > 0.5\n",
    "print('Accuracy of the trained Sigmoid', accuracy(ys_for_su, hat_ys))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Networks in PyTorch\n",
    "\n",
    "PyTorch is an open source machine learning library based on the Torch library, used for applications such as computer\n",
    "vision and natural language processing, primarily developed by Facebook's AI Research lab.\n",
    "\n",
    "PyTorch is a scientific computing package serving two broad purposes:\n",
    "* A replacement for NumPy to use the power of GPUs and other accelerators.\n",
    "* An automatic differentiation library that is useful to implement neural networks.\n",
    "\n",
    "To correctly install PyTorch, please follow this [link](https://pytorch.org/).\n",
    "\n",
    "## The MNIST Dataset\n",
    "\n",
    "The MNIST dataset is a large database of handwritten digits.\n",
    "The MNIST database contains 60,000 training images and 10,000 testing images.\n",
    "The images are grayscale, 28x28 pixels, and centered to reduce preprocessing and get started quicker."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# download train and test sets\n",
    "dataset_train = datasets.MNIST(root = './data', train = True, transform = transforms.ToTensor(), download = True)\n",
    "dataset_validation = datasets.MNIST(root = './data', train = False, transform = transforms.ToTensor())\n",
    "\n",
    "print(\"Number of examples in the training set:\", dataset_train)\n",
    "print(\"Number of examples in the validation set:\", dataset_validation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's start by visualising the first batch of examples."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get the first 8 training images\n",
    "images = dataset_train.train_data[:8].reshape(-1, 1, 28, 28)\n",
    "labels = dataset_train.train_labels[:8]\n",
    "\n",
    "# show images\n",
    "imshow(make_grid(images))\n",
    "# print labels\n",
    "print(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now print some statistics about the images above and the dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Shape of the input:', images.shape)\n",
    "print('Shape of the output:', labels.shape)\n",
    "print('Number of classes:', len(dataset_train.classes))\n",
    "print('Classes:', dataset_train.classes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "PyTorch implements a way to handle training data using the DataLoader class. We will use this in order to\n",
    "iterate across the training and validation sets. The batch size is the number of examples we will provide to the model\n",
    "at every iteration. This usually corresponds to the batch size of GD."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "# use the dataloader helper to generate an iterator for the train set to return random examples 5 by 5 (train batch size).\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "# use the dataloader helper to generate an iterator for the test set to return examples 100 by 100.\n",
    "validation_loader = DataLoader(dataset_validation, batch_size=100, shuffle=False, num_workers=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multi-Layer Perceptron in PyTorch\n",
    "\n",
    "The task you will solve is: given a handwritten digit, from 0 to 9, recognize and output its digit.\n",
    "\n",
    "This task is not a task for binary classifiers because the output required by the classifier is not binary but decimal.\n",
    "This will require the definition of a so called multi-way classifier, a 10-way classifier in this case.\n",
    "You can think of a 10-way classifier as 10 independent binary classifiers, one for each digit, which all at the same\n",
    "time output a prediction about the probability of having recognized a digit or not.\n",
    "Then, take the most confident classifier as the correct answer.\n",
    "\n",
    "We will now define and train a Multi-Layer Perceptron. In PyTorch you can do this in two ways:\n",
    "* the easy but limited way, or;\n",
    "* the hard but flexible way.\n",
    "\n",
    "Let's start with the hard way. Here you need to create a class with a constructor used to define the components of\n",
    "the neural network, and a forward method used to combine the parts of the network together. This computes also the\n",
    "forward step of the network. There is no need to define the backward step used for training because fully handled by\n",
    "PyTorch."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size, hidden_sizes, output_size):\n",
    "    super(MLP, self).__init__()\n",
    "    self.layers = nn.ModuleList() # list of layers\n",
    "    next_hidden_size = input_size # input size of the first layer, this depends on the dataset\n",
    "    for hidden_size in hidden_sizes: # for loop to define the next hidden layers\n",
    "      self.layers.append(nn.Linear(next_hidden_size, hidden_size))\n",
    "      next_hidden_size = hidden_size\n",
    "    self.output = nn.Linear(next_hidden_size, output_size) # output layer\n",
    "\n",
    "  def forward(self, x): # defines the forward pass\n",
    "    out = x\n",
    "    for layer in self.layers:\n",
    "        out = layer(out)\n",
    "        out = F.relu(out) # activation function\n",
    "    out = self.output(out)\n",
    "    return out\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now you can instantiate this model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = MLP(28*28, [400, 400, 10], 10)\n",
    "\n",
    "print(\"Print the structure of the model:\")\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will now define the same module in the easy way. Here you just need to use components of PyTorch to construct the\n",
    "network by stacking elements on top of each other using the Sequential class."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(28*28, 400), # input layer\n",
    "    nn.ReLU(), # activation function\n",
    "    nn.Linear(400, 400), # first hidden layer\n",
    "    nn.ReLU(), # activation function\n",
    "    nn.Linear(400, 400), # second hidden layer\n",
    "    nn.ReLU(), # activation function\n",
    "    nn.Linear(400, 10)) # output layer\n",
    "\n",
    "print(\"Print the structure of the model:\")\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have a model, we can select the optimizer. For this exercise we use Stochastic Gradient Descent (SGD).\n",
    "To do this we need to pass which parameters of our model need to be optimized, a learning rate and a momentum."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.99)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For an MLP in a multi-way classification setting a good loss is the cross entropy.\n",
    "In the next code cell we will define this loss."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "criterium = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will now define the training loop.\n",
    "\n",
    "But first let's define a helper function used to compute the loss of the model on the validation set while training."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_loss_validation(model):\n",
    "    tot_loss = 0\n",
    "    batch_size = validation_loader.batch_size\n",
    "    for i, batch in enumerate(validation_loader):\n",
    "        images, labels = batch\n",
    "        xs = images.view(-1, 28*28)\n",
    "        ys = labels\n",
    "\n",
    "        pred_ys = model(xs) # generate the predictions using the model\n",
    "        loss = criterium(pred_ys, ys) # evaluate the predictions using the cross entropy loss\n",
    "        tot_loss += loss.item() # get the number and sum it to the total loss\n",
    "\n",
    "        if (i+1) % batch_size == 0:\n",
    "            break\n",
    "\n",
    "    loss = tot_loss / batch_size # normalize the loss based on the number of testing examples\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now set the number of times the model will be trained on the entire training set."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = 10 # how many times you want to train on the dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now you will first set the model to training mode. This means that the provided examples will be used for training.\n",
    "Then, the model will loop over two for loops, one over the epochs and one over the batches."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.train() # set the model to training mode\n",
    "\n",
    "# these variables are used to store the losses\n",
    "running_loss = 0\n",
    "training_loss = []\n",
    "validating_loss = []\n",
    "# loop over the epochs\n",
    "for epoch in range(epochs):\n",
    "  # loop over the batches\n",
    "  for i, batch in enumerate(train_loader):\n",
    "    images, labels = batch # extract images and labels\n",
    "    xs = images.view(-1, 28*28) # flatten the image into a vector\n",
    "    ys = labels\n",
    "\n",
    "    optimizer.zero_grad() # reset the gradients\n",
    "    pred_ys = model(xs) # generate the predictions\n",
    "    loss = criterium(pred_ys, ys) # compute the loss\n",
    "    loss.backward() # backpropagation\n",
    "    optimizer.step() # optmizes here\n",
    "\n",
    "    running_loss += loss.item()\n",
    "    if (i+1) % 200 == 0: # every 100 batches print statistics about the training\n",
    "      running_loss /= 200 # training loss on the last batch\n",
    "      training_loss.append(running_loss) # keep track of the training loss\n",
    "      model.eval() # set the model to eval, we can now test the model\n",
    "      validation_loss = compute_loss_validation(model) # compute the validation loss\n",
    "      validating_loss.append(validation_loss) # keep track of the validation loss\n",
    "      model.train() # set back the model to train\n",
    "      print('Epoch [%d/%d], Step [%d/%d], Train Loss: %.4f, Validation Loss: %.4f'%(\n",
    "          epoch+1,\n",
    "          epochs,\n",
    "          i+1,\n",
    "          len(dataset_train)//batch_size, running_loss, validation_loss))\n",
    "      running_loss = 0\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can now plot the train and validation losses. However, in order to get a better plot we need to shift the\n",
    "training loss by one. This because, although the training loss and validation losses are computed over 200 batches,\n",
    "the training loss is used to improve the model. This means that its loss decreases across the batches, while\n",
    "the validation loss does not, because computed with the current best model. Hence, the validation loss may\n",
    "become lower than the training loss. To avoid this artifact, we shift the validation loss by one forward."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_loss.pop()\n",
    "validating_loss.insert(0, float('NaN'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now plot these losses."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(training_loss, label=\"train loss\")\n",
    "plt.plot(validating_loss, label=\"validation loss\")\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "for i in range(epochs):\n",
    "    plt.axvline(x=(len(dataset_train)//batch_size)*(i+1)/200,color='gray')\n",
    "\n",
    "plt.ylabel('loss');\n",
    "plt.xlabel('batch')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can evaluate this classifier using Accuracy. This is defined in the following code cell."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def accuracy(data_loader):\n",
    "    correct = 0.0 # here you will count the correct answers\n",
    "    total = 0.0 # here you will count all the answers\n",
    "    with torch.no_grad(): # ingnore the gradient graph\n",
    "        for batch in data_loader:\n",
    "            images, labels = batch\n",
    "            xs = images.view((-1, 28*28))\n",
    "            ys = labels\n",
    "\n",
    "            hat_ys = model(xs).detach().cpu()\n",
    "            _, hat_ys = torch.max(hat_ys, 1)\n",
    "            correct += (hat_ys == ys).sum()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return correct/total"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can now compute the accuracy of the model on the train and validation sets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval() # set the model to evaluation\n",
    "\n",
    "train_accuracy = accuracy(train_loader)\n",
    "print('Train accuracy of the MLP {:.3f}'.format(train_accuracy))\n",
    "\n",
    "validation_accuracy = accuracy(validation_loader)\n",
    "print('Validation accuracy of the MLP {:.3f}'.format(validation_accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}