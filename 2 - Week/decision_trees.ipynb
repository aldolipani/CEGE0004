{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Decision Trees\n",
    "\n",
    "In this notebook you will learn how to implement the ID3 decision tree learner, reproduce the results presented in the\n",
    "lecture, and learn how to use the decision tree learners of a popular machine learning module, scikit-learn.\n",
    "\n",
    "## The Dataset\n",
    "\n",
    "In the following code cell, we will define a toy-dataset for the PlayTennis learning task. This dataset consists of\n",
    "4 discrete features: outlook, temperature, humidity and wind. The values that each of these attributes can take are\n",
    "defined in the `features` dictionary. Each key of this dictionary corresponds to a tuple where we store the feature\n",
    "name, and the position of that feature in the training examples.\n",
    "The `xs` list defines all the examples of this dataset, and in\n",
    "the `ys` list we find the corresponding target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "features = {('outlook', 0):\n",
    "                {'sunny', 'overcast', 'rain'},\n",
    "            ('temperature', 1):\n",
    "                {'hot', 'mild', 'cool'},\n",
    "            ('humidity', 2):\n",
    "                {'high', 'normal', 'low'},\n",
    "            ('wind', 3):\n",
    "                {'weak', 'strong'}}\n",
    "\n",
    "xs = [['sunny', 'hot', 'high', 'weak'],\n",
    "      ['sunny', 'hot', 'high', 'strong'],\n",
    "      ['overcast', 'hot', 'high', 'weak'],\n",
    "      ['rain', 'mild', 'high', 'weak'],\n",
    "      ['rain', 'cool', 'normal', 'weak'],\n",
    "      ['rain', 'cool', 'normal', 'strong'],\n",
    "      ['overcast', 'cool', 'normal', 'strong'],\n",
    "      ['sunny', 'mild', 'high', 'weak'],\n",
    "      ['sunny', 'cool', 'high', 'weak'],\n",
    "      ['rain', 'mild', 'normal', 'weak'],\n",
    "      ['sunny', 'mild', 'normal', 'strong'],\n",
    "      ['overcast', 'mild', 'high', 'strong'],\n",
    "      ['overcast', 'hot', 'normal', 'weak'],\n",
    "      ['rain', 'mild', 'high', 'strong']]\n",
    "\n",
    "ys = ['no', 'no', 'yes', 'yes', 'yes', 'no', 'yes', 'no', 'yes', 'yes', 'yes', 'yes', 'yes', 'no']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Tree Data Structure\n",
    "\n",
    "We define decision trees using a `Node` class which will be used to represent both internal and leaf nodes.\n",
    "This class also implements two methods `is_leaf` and `predict`, which allow us to check if the node is an internal node\n",
    "or a leaf node, and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.feature = None\n",
    "        self.children = {}\n",
    "        self.label = None\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return self.label is not None\n",
    "\n",
    "    def predict(self, x):\n",
    "        if self.is_leaf():\n",
    "            return self.label\n",
    "        else:\n",
    "            feature_i = self.feature[1]\n",
    "            return self.children[x[feature_i]].predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With the following code we construct the decision tree seen in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "no = Node()\n",
    "no.label = 'no'\n",
    "\n",
    "yes = Node()\n",
    "yes .label = 'yes'\n",
    "\n",
    "humidity = Node()\n",
    "humidity.feature = ('humidity', 2)\n",
    "humidity.children['high'] = no\n",
    "humidity.children['normal'] = yes\n",
    "\n",
    "wind = Node()\n",
    "wind.feature = ('wind', 3)\n",
    "wind.children['strong'] = no\n",
    "wind.children['weak'] = yes\n",
    "\n",
    "outlook = Node()\n",
    "outlook.feature = ('outlook', 0)\n",
    "outlook.children['sunny'] = humidity\n",
    "outlook.children['overcast'] = yes\n",
    "outlook.children['rain'] = wind\n",
    "\n",
    "play_tennis = outlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In order to see the tree we implement a `show` function that displays it in textual form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def show(node, skip = ''):\n",
    "    if node.feature:\n",
    "        print(skip, node.feature[0] + ':')\n",
    "    else:\n",
    "        print(skip, '->', node.label)\n",
    "    for value, node in node.children.items():\n",
    "        print(skip, '->', value)\n",
    "        show(node, skip + '  ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Does it look as expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "show(play_tennis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To use this decision tree we can use the `predict` method defined in the node class. This function requires a list of\n",
    "values ordered as defined in the `features` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "play_tennis.predict(['sunny', 'hot', 'normal', 'weak'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The ID3 Algorithm\n",
    "\n",
    "We will now implement the ID3 algorithm in Python from scratch. As we have seen in the lecture material,\n",
    "the ID3 algorithm requires the use of 3 helper functions:\n",
    "1. one that allow the implementation of a measure to select the best feature, `select_best_feature`;\n",
    "2. one to select the data based on a feature and feature value, `select_data`, and;\n",
    "3. one to remove one feature from the list of considered features, `dict_minus`.\n",
    "\n",
    "We start by implementing a measure to select the best feature randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def select_best_feature(xs, ys, features):\n",
    "    return random.choice(list(features.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can try this function yourself. This function will always return a different feature, regardless of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random_feature = select_best_feature(xs, ys, features)\n",
    "random_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then, we implement the `select_data` function used to partition the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def select_data(xs, ys, feature, value):\n",
    "    xs_value = []\n",
    "    ys_value = []\n",
    "    for x, y in zip(xs, ys):\n",
    "        if x[feature[1]] == value:\n",
    "            xs_value.append(x)\n",
    "            ys_value.append(y)\n",
    "\n",
    "    return xs_value, ys_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now test this function by selecting the examples with value hot for the feature temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "select_data(xs, ys, ('temperature', 1), 'hot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This last function just returns a new dictionary, identical to the original one, but with one key removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def dict_minus(dict, key):\n",
    "    dict = dict.copy()\n",
    "    dict.pop(key)\n",
    "    return dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "example_dict = {'A': 'I need to go', 'B': 'I need to stay', 'C': 'Me too'}\n",
    "example_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dict_minus(example_dict, 'A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can now implement the ID3 learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from statistics import mode\n",
    "\n",
    "def id3(xs, ys, features):\n",
    "    root = Node()\n",
    "    if all([y == 'yes' for y in ys]): root.label = 'yes'\n",
    "    elif all([y == 'no' for y in ys]): root.label = 'no'\n",
    "    elif not features: root.label = mode(ys)\n",
    "    else:\n",
    "        feature = select_best_feature(xs, ys, features)\n",
    "        root.feature = feature\n",
    "        for value in features[feature]:\n",
    "            node = Node()\n",
    "            root.children[value] = node\n",
    "            xs_value, ys_value = select_data(xs, ys, feature, value)\n",
    "            if not xs_value: node.label = mode(ys)\n",
    "            else: root.children[value] = \\\n",
    "                id3(xs_value, ys_value, dict_minus(features, feature))\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To learn a decision tree based on the training set we will just need to invoke this function with the right parameters.\n",
    "\n",
    "Remember that we are now using an unusual `select_best_feature` (it picks an attribute at random). Will the\n",
    "learning algorithm output a decision tree that fits the training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tree_ran = id3(xs, ys, features)\n",
    "\n",
    "show(tree_ran)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's now implement a better heuristic (than random) to select the best feature. We implement the one based on the error\n",
    "rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def select_best_feature(xs, ys, features):\n",
    "    res = None\n",
    "    min_err = None\n",
    "    for feature in features:\n",
    "        err = 0\n",
    "        for value in features[feature]:\n",
    "            xs_value, ys_value = select_data(xs, ys, feature, value)\n",
    "            if ys_value:\n",
    "                y_pred = mode(ys_value)\n",
    "                err += sum([y_pred != y for y in ys_value])\n",
    "\n",
    "        if min_err is None or err < min_err:\n",
    "            min_err = err\n",
    "            res = feature\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's run this function on the original dataset and see that the feature it will select is now humidity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "select_best_feature(xs, ys, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tree_err = id3(xs, ys, features)\n",
    "show(tree_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Decision Trees in Scikit-Learn\n",
    "\n",
    "Scikit-learn is a free software machine learning library for Python. It features various classification, regression and\n",
    "clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and\n",
    "is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. In this notebook you\n",
    "will learn how to use the decision trees implemented in scikit-learn.\n",
    "\n",
    "Before you start, yous should make sure to have scikit-learn installed in your anaconda environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!conda install --yes scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The Dataset\n",
    "\n",
    "In this notebook you will be working with the Iris dataset. The Iris flower data set or Fisher's Iris data set is a\n",
    "multivariate data set introduced by the British statistician, eugenicist, and biologist Ronald Fisher in his 1936 paper:\n",
    "\"The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis\".\n",
    "\n",
    "The iris dataset is a classic and easy multi-class classification dataset. In the following table we find some of the\n",
    "properties of this dataset.\n",
    "\n",
    "| Property | Value |\n",
    "|--|--|\n",
    "| Classes | 3 |\n",
    "| Samples per class | 50 |\n",
    "| Samples total | 150 |\n",
    "| Dimensionality | 4 |\n",
    "| Features | real, positive |\n",
    "\n",
    "Each example contains the measurements of four features of iris plants: the length and the width of the sepals and\n",
    "petals, in centimeters; and, each example is labeled with one of three classes corresponding to one of the iris species:\n",
    "iris setosa, iris virginica and iris versicolor.\n",
    "\n",
    "Scikit-learn provides a set of famous datasets, and the Iris dataset is one of them. To load the Iris dataset we just\n",
    "need to run the function `load_iris`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now define the dataset we will be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "xs = iris.data\n",
    "ys = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Learning a Decision Tree\n",
    "\n",
    "Scikit-learn implements a `DecisionTreeClassifier` for classification tasks and a `DecisionTreeRegressor` for regression\n",
    "tasks. In this notebook we will learn to use the `DecisionTreeClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A `DecisionTreeClassifier` object takes as input two arrays: an array `xs`, sparse or dense, of shape\n",
    "(n_samples, n_features) holding the training samples, and an array `ys` of integer values, shape (n_samples,),\n",
    "holding the class labels for the training samples.\n",
    "\n",
    "Read more about the property of this class directly from the documentation of scikit-learn following this\n",
    "[link](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier).\n",
    "\n",
    "To train this classifier with default parameters you just need to create a `DecisionTreeClassifier` object and invoke\n",
    "the method fit on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "toy_xs = [[0, 0], [1, 1]]\n",
    "toy_ys = ['yes', 'no']\n",
    "\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "\n",
    "tree_clf.fit(toy_xs, toy_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can now use the method predict to test this tree on a new example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tree_clf.predict([[0, 1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can also visualize the tree by using the method `plot_tree`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plot_tree(tree_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "What we can see here is that this tree rather than using the Entropy uses the gini index. We can change that by changing\n",
    "the `criterion` parameter of the decision tree.\n",
    "\n",
    "We now train a decision tree on the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(criterion='entropy', max_depth=2)\n",
    "tree_clf.fit(xs, ys)\n",
    "\n",
    "plot_tree(tree_clf, feature_names=iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Evaluating the Decision Tree\n",
    "\n",
    "To evaluate the performance of the decision tree trained we use the accuracy measure. The accuracy measure tells us\n",
    "what is the proportion of the dataset that has been correctly classified.\n",
    "\n",
    "Follow this\n",
    "[link](https://developers.google.com/machine-learning/crash-course/classification/accuracy)\n",
    "to learn more about this measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_true=ys, y_pred=tree_clf.predict(xs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Decision Boundary\n",
    "\n",
    "In the following we want to visualize the decision boundary of a learned decision tree for the Iris dataset. To make\n",
    "the visualization readable we do this for feature pairs. We select these pairs via\n",
    "the variables `axis_0`, and `axis_1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# let's train a decision tree only on two features\n",
    "axis_0 = 0\n",
    "axis_1 = 1\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(criterion='entropy', max_depth=2)\n",
    "tree_clf.fit(xs[:, [axis_0, axis_1]], ys)\n",
    "\n",
    "# create a grid of points to plot the countour\n",
    "x_min, x_max = xs[:, 0].min() - 1, xs[:, 0].max() + 1\n",
    "y_min, y_max = xs[:, 1].min() - 1, xs[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "\n",
    "# predict the outcome for the grid of points\n",
    "zz = tree_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "zz = zz.reshape(xx.shape)\n",
    "cs = plt.contourf(xx, yy, zz, cmap=plt.cm.RdYlBu)\n",
    "\n",
    "# define axis\n",
    "plt.xlabel(iris.feature_names[axis_0])\n",
    "plt.ylabel(iris.feature_names[axis_1])\n",
    "\n",
    "# plot the dataset\n",
    "for i, color, marker in zip(range(3), 'ryb', 'o^s'):\n",
    "    idx = np.where(ys == i)\n",
    "    plt.scatter(xs[idx, axis_0], xs[idx, axis_1], c=color, marker=marker, label=iris.target_names[i],\n",
    "                cmap=plt.cm.RdYlBu, edgecolor='black', s=15)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
